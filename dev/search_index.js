var documenterSearchIndex = {"docs":
[{"location":"tips/#tips-1","page":"Tips","title":"Tips","text":"","category":"section"},{"location":"tips/#","page":"Tips","title":"Tips","text":"This page of the documentation holds miscellaneous tips for using the package.","category":"page"},{"location":"tips/#ccgf-tips-1","page":"Tips","title":"Deriving the conditional cumulant generating function","text":"","category":"section"},{"location":"tips/#","page":"Tips","title":"Tips","text":"The cumulant generating function is based upon the moment-generating function. If","category":"page"},{"location":"tips/#","page":"Tips","title":"Tips","text":"beginaligned\nM_X(t) equiv mathbbEe^tXquad quad quad tin mathbbR\nendaligned","category":"page"},{"location":"tips/#","page":"Tips","title":"Tips","text":"is the moment-generating function of a random variable X, then the cumulant-generating function is just","category":"page"},{"location":"tips/#","page":"Tips","title":"Tips","text":"beginaligned\nccgf_X(t) equiv logmathbbEe^tXquad quad quad tin mathbbR\nendaligned","category":"page"},{"location":"tips/#","page":"Tips","title":"Tips","text":"As an example, if X sim N(mu sigma^2), then M_X(t) = exp(tmu + sigma^2 t^2  2) and ccgf_X(t) = tmu + sigma^2 t^2  2.","category":"page"},{"location":"tips/#","page":"Tips","title":"Tips","text":"Risk-adjusted linearizations imply that the relative entropy measure mathcalV(Gamma_5 z_t + 1 + Gamma_6 y_t + 1) becomes a vector of conditional cumulant-generating functions for the random variables A_i(z_t) varepsilon_t + 1, where A_i(z_t) is the ith row vector of","category":"page"},{"location":"tips/#","page":"Tips","title":"Tips","text":"beginaligned\nA(z_t) = (Gamma_5 + Gamma_6 Psi)(I_n_z - Lambda(z_t) Psi)^-1 Sigma(z_t)\nendaligned","category":"page"},{"location":"tips/#","page":"Tips","title":"Tips","text":"To create a RiskAdjustedLinearization, the user needs to define a function ccgf in the form ccgf(F, A, z) or ccgf(A, z), where A refers to the matrix A(z_t) once it has already been evaluated at z_t. In other words, the input A should seen as a n_y times n_varepsilon matrix of real scalars. However, depending on the distributions of the martingale difference sequence varepsilon_t + 1, writing the conditional cumulant-generating function may also require knowing the current state z_t.","category":"page"},{"location":"tips/#","page":"Tips","title":"Tips","text":"Let us consider two didactic examples. First, assume varepsilon_t + 1sim mathcalN(0 I). Then we claim","category":"page"},{"location":"tips/#","page":"Tips","title":"Tips","text":"ccgf(A, z) = sum(A.^2, dims = 2) / 2","category":"page"},{"location":"tips/#","page":"Tips","title":"Tips","text":"Based on the definition of mathcalV(z_t), one may be tempted to derive the conditional cumulant-generating function for the random vector A(z_t) varepsilon_t + 1. However, this is not actually what we want. Rather, ccgf should just return a vector of conditional cumulant-generating functions for the n_y random variables X_i = A_i(z_t)varepsilon_t + 1.","category":"page"},{"location":"tips/#","page":"Tips","title":"Tips","text":"Because the individual components of varepsilon_t + 1 are independent and each varepsilon_i t has a standard Normal distribution, the moment-generating function for X_i is expleft(frac12left(sum_j = 1^n_varepsilon (t A_ij)^2  2right)right), hence the ith cumulant-generating function is frac12left(sum_j = 1^n_varepsilon (t A_ij)^2  2right). For risk-adjusted linearizations, we evaluate at t = 1 since we want the conditional cumulant-generating function logmathbbE_texp(A_i(z_t)varepsilon_t + 1). This is precisely what the code above achieves.","category":"page"},{"location":"tips/#","page":"Tips","title":"Tips","text":"Second, let us consider a more complicated example. In the Wachter (2013) Example, the ccgf is","category":"page"},{"location":"tips/#","page":"Tips","title":"Tips","text":"function ccgf(F, Œ±, z) # Œ± is used here instead of A\n    # the S term in z[S[:p]] is just an ordered dictionary mapping the symbol :p to the desired index of z\n    F .= .5 .* Œ±[:, 1].^2 + .5 * Œ±[:, 2].^2 + (exp.(Œ±[:, 3] + Œ±[:, 3].^2 .* Œ¥^2 ./ 2.) .- 1. - Œ±[:, 3]) * z[S[:p]]\nend","category":"page"},{"location":"tips/#","page":"Tips","title":"Tips","text":"Observe that the first two quantities .5 .* Œ±[:, 1].^2 + .5 * Œ±[:, 2].^2 resemble what would be obtained from a standard multivariate normal distribution. The remaining terms are more complicated because the Wachter (2013) model involves a Poisson mixture of normal distributions. It will be instructive to spell the details out.","category":"page"},{"location":"tips/#","page":"Tips","title":"Tips","text":"Consumption growth follows the exogenous process","category":"page"},{"location":"tips/#","page":"Tips","title":"Tips","text":"beginaligned\nc_t + 1 = mu + c_t + sigma varepsilon^c_t + 1 - theta xi_t + 1\nendaligned","category":"page"},{"location":"tips/#","page":"Tips","title":"Tips","text":"where varepsilon_t^c sim N(0 1) is iid over time and xi_t mid j_t sim N(j_t j_tdelta^2), where the number of jumps j_t sim Poisson(p_t - 1), hence mathbbE_t xi_t + 1 = mathbbE_t j_t + 1 = p_t. Assume that varepsilon_t^c and varepsilon_t^xi = xi_t - mathbbE_t - 1xi_t are independent. Finally, the intensity p_t follows the process","category":"page"},{"location":"tips/#","page":"Tips","title":"Tips","text":"beginaligned\np_t + 1 = (1 - rho_p) p + rho_p p_t + sqrtp_t phi_p sigma varepsilon_t + 1^p\nendaligned","category":"page"},{"location":"tips/#","page":"Tips","title":"Tips","text":"where varepsilon_t^p sim N(0 1) is iid over time and independent of varepsilon_t^c and varepsilon_t^xi.","category":"page"},{"location":"tips/#","page":"Tips","title":"Tips","text":"Note that xi_t and mathbbE_t - 1xi_t are not independent because mathbbE_t - 1xi_t = p_t - 1 and j_t sim Poisson(p_t - 1), hence a higher p_t - 1 implies xi_t is more likely to be higher. Re-centering xi_t by mathbbE_t - 1xi_t creates a martingale difference sequence since xi_t mid j_t is normal.","category":"page"},{"location":"tips/#","page":"Tips","title":"Tips","text":"By independence of the components of varepsilon_t = varepsilon_t^c varepsilon_t^p varepsilon_t^xi^T, the conditional cumulant-generating function for the ith row of the A(z_t) matrix described in this section is","category":"page"},{"location":"tips/#","page":"Tips","title":"Tips","text":"beginaligned\nccgf_iA_i(z_t) mid z_t   =  logmathbbE_texp(A_i1(z_t) varepsilon_t + 1^c)  + logmathbbE_texp(A_i2(z_t) varepsilon_t + 1^p) + logmathbbE_texp(A_i3(z_t) varepsilon_t + 1^xi)\nendaligned","category":"page"},{"location":"tips/#","page":"Tips","title":"Tips","text":"The first two terms on the RHS are for normal random variables and simplify to (A_i1(z_t)^2 + A_i2(z_t)^2)  2. To calculate the remaining term, note that mathbbE_txi_t + 1 = p_t is already part of the information set at z_t, hence","category":"page"},{"location":"tips/#","page":"Tips","title":"Tips","text":"beginaligned\nlogmathbbE_texp(A_i3(z_t) varepsilon_t + 1^xi)  = logleftfrac1exp(A_i3(z_t) p_t)mathbbE_tleftexp(A_i3(z_t) xi_t + 1)rightright \n                                   = logmathbbE_tleftexp(A_i3(z_t) xi_t + 1)right - A_i3(z_t) p_t\nendaligned","category":"page"},{"location":"tips/#","page":"Tips","title":"Tips","text":"To calculate the cumulant-generating function of xi_t, aside from direct calculation, we can also use the results for mixture distributions in Villa and Escobr (2006) or Bagui et al. (2020). Given random variables X and Y, assume the conditional distribution Xmid Y and the marginal distribution for Y are available. If we can write the moment-generating function for the random variable Xmid Y as","category":"page"},{"location":"tips/#","page":"Tips","title":"Tips","text":"beginaligned\nM_X mid Y(s) = C_1(s) exp(C_2(s) Y)\nendaligned","category":"page"},{"location":"tips/#","page":"Tips","title":"Tips","text":"then the moment-generating function of X is","category":"page"},{"location":"tips/#","page":"Tips","title":"Tips","text":"beginaligned\nM_X(s) = C_1(s) M_YC_2(s)\nendaligned","category":"page"},{"location":"tips/#","page":"Tips","title":"Tips","text":"In our case, we have","category":"page"},{"location":"tips/#","page":"Tips","title":"Tips","text":"beginaligned\nM_xi_t mid j_t(s) = expleft(s j_t  + frac12 s^2 delta^2j_t  right)\nendaligned","category":"page"},{"location":"tips/#","page":"Tips","title":"Tips","text":"hence C_1(s) = 0 and C_2(s) = (s + s^2delta^2  2). The variable j_t has a Poisson distribution with intensity p_t, which implies the moment-generating function","category":"page"},{"location":"tips/#","page":"Tips","title":"Tips","text":"beginaligned\nM_j_t(s) = exp((exp(s) - 1) p_t)\nendaligned","category":"page"},{"location":"tips/#","page":"Tips","title":"Tips","text":"Thus, as desired,","category":"page"},{"location":"tips/#","page":"Tips","title":"Tips","text":"beginaligned\nlogmathbbE_tleftexp(A_i3(z_t) xi_t + 1)right - A_i3(z_t) p_t  = (exp(A_i3(z_t)  + A_i3(z_t)^2delta^2) - 1)p_t - A_i3(z_t) p_t\nendaligned","category":"page"},{"location":"tips/#","page":"Tips","title":"Tips","text":"Computing this quantity for each expectational equation yields the ccgf used in the Wachter (2013) Example.","category":"page"},{"location":"tips/#Writing-functions-compatible-with-automatic-differentiation-1","page":"Tips","title":"Writing functions compatible with automatic differentiation","text":"","category":"section"},{"location":"tips/#","page":"Tips","title":"Tips","text":"Use an in-place function to avoid type errors.\nFor example, define the ccgf as ccgf(F, x). You can use the element type of F via eltype(F) to ensure that you don't get a type error from using Float64 instead of Dual inside the function. If ccgf was out-of-place, then depending on how the vector being returned is coded, you may get a type error if elements of the return vector are zero or constant numbers. By having F available, you can guarantee these numbers can be converted to Dual types if needed without always declaring them as Dual types.","category":"page"},{"location":"tips/#","page":"Tips","title":"Tips","text":"Use dualvector or dualarray.\nThe package provides these two helper functions in the case where you have a function f(x, y), and you need to be able to automatcally differentiate with respect to x and y separately. For example, the nonlinear terms of the expectational equation Œæ(z, y) takes this form. Within , you can pre-allocate the return vector by calling F = RiskAdjustedLinearizations.dualvector(z, y). The dualvector function will infer from z and y whether F should be have Dual element types or not so you can repeatedly avoid writing if-else conditional blocks. The dualarray function generalizes this to arbitrary AbstractMatrix inputs. See the out-of-place function for Œæ in examples/wachter_disaster_risk/wachter.jl.","category":"page"},{"location":"tips/#","page":"Tips","title":"Tips","text":"Don't pre-allocate the return vector.\nInstead of pre-allocating the return vector at the  top of the function for an out-of-place function, just concatenate the individual elements  at the very end. Julia will figure out the appropriate element type for you. The downside of this  approach is that you won't be able to assign names to the specific indices of the return vector (e.g.  does this equation define the risk-free interest rate?). For small models, this disadvantage is generally not a problem.  See the definition of the out-of-place expected state transition function Œº in examples/wachter_disaster_risk/wachter.jl.\nExponentiate all terms to write conditions in levels.","category":"page"},{"location":"tips/#","page":"Tips","title":"Tips","text":"Automatic differentiation will be faster if the equilibrium conditions are written in the form","category":"page"},{"location":"tips/#","page":"Tips","title":"Tips","text":"beginaligned\n0  = log(F(x))\nendaligned","category":"page"},{"location":"tips/#","page":"Tips","title":"Tips","text":"instead of in levels","category":"page"},{"location":"tips/#","page":"Tips","title":"Tips","text":"beginaligned\n1  = F(x)\nendaligned","category":"page"},{"location":"tips/#","page":"Tips","title":"Tips","text":"However, it may be easier and/or less error prone to write the equilibrium conditions in levels. This approach can be easily accomplished by (1) exponentiating all input arguments at the beginning of the function to convert inputs from logs to levels; (2) writing all equilibrium conditions in the form 1 = F(x); and (3) returning the output as log(F(x)).","category":"page"},{"location":"diagnostics/#diagnostics-1","page":"Diagnostics","title":"Diagnostics","text":"","category":"section"},{"location":"diagnostics/#","page":"Diagnostics","title":"Diagnostics","text":"To assess the quality of a risk-adjusted linearization, diagnostic tests should be run. In particular, as Lopez et al. (2018) discuss at length, whenever forward difference equations arise (e.g. the equation for the log wealth-consumption ratio in our implementation of Wachter (2013)), there are infinitely many ways to write the expectational equations. Assuming computational costs do not become too significant, users should add as many expectational equations as needed to maximize the accuracy of the risk-adjusted linearization.","category":"page"},{"location":"diagnostics/#","page":"Diagnostics","title":"Diagnostics","text":"The best accuracy test is comparing the risk-adjusted linearization to the true nonlinear solution, but this test requires this solution to be available. In many cases (e.g. high dimensions), neither analytical nor numerical methods can deliver the true solution. To address this problem, economists have developed a variety of accuracy tests that only involve the chosen approximation method and quadrature.","category":"page"},{"location":"diagnostics/#","page":"Diagnostics","title":"Diagnostics","text":"The most popular diagnostics revolve around the Euler equation. RiskAdjustedLinearizations.jl implements wrapper functions for performing two of these Euler equation diagnostics. The first is the so-called \"Euler equation errors\" test proposed by Judd (1992). The second is the so-called \"dynamic Euler equation errors\" test proposed by Den Haan (2009). We defer the reader to these articles for explanations of the theory behind these tests. A good set of slides on accuracy tests are these ones by Den Haan.","category":"page"},{"location":"diagnostics/#","page":"Diagnostics","title":"Diagnostics","text":"The wrapper functions in RiskAdjustedLinearizations.jl are euler_equation_error and dynamic_euler_equation_error. See the Coeurdacier, Rey, and Winant (2011) script for an example of how to use these functions.","category":"page"},{"location":"diagnostics/#","page":"Diagnostics","title":"Diagnostics","text":"RiskAdjustedLinearizations.euler_equation_error\nRiskAdjustedLinearizations.dynamic_euler_equation_error","category":"page"},{"location":"diagnostics/#RiskAdjustedLinearizations.euler_equation_error","page":"Diagnostics","title":"RiskAdjustedLinearizations.euler_equation_error","text":"euler_equation_error(m, c‚Çú, logSDFxR, ùîº_quadrature, z‚Çú = m.z;\n    c_init = NaN, return_soln = false, kwargs...)\neuler_equation_error(m, c‚Çú, logSDFxR, ùîº_quadrature, shock_matrix, z‚Çú = m.z;\n    c_init = NaN, summary_statistic = x -> norm(x, Inf), burnin = 0,\n    return_soln = false, kwargs...)\n\ncalculates standard Euler equation errors, as recommended by Judd (1992). The first method calculates the error at some state z‚Çú, which defaults to the stochastic steady state. The second method simulates the state vector from an initial state z‚Çú (defaults to stochastic steady state) given a sequence of drawn shocks, evaluates the Euler equation errors, and returns some summary statistic of the errors specified by the keyword summary_statistic.\n\nThe Euler equation is\n\nbeginaligned\n0 = log mathbbE_t exp(m_t + 1 + r_t + 1) = log mathbbE_tM_t + 1 R_t + 1\nendaligned\n\nwhere m_t + 1 = log(M_t + 1) is the log stochastic discount factor and r_t + 1 = log(R_t + 1) is the risk free rate.\n\nInputs\n\nm::RiskAdjustedLinearization: A solved instance of a risk-adjusted linearization\nc‚Çú::Function: a function of (m, z‚Çú) that calculates consumption at state z‚Çú, given the   state-space representation implied by m.\nlogSDFxR::Function: a Function evaluating m_t + 1 + r_t + 1. The Function must   take as input (m, z‚Çú, Œµ‚Çú‚Çä‚ÇÅ, c), where m is a RiskAdjustedLinearization,   z‚Çú is a state vector at which to evaluate, Œµ‚Çú‚Çä‚ÇÅ is a draw from the distribution   of exogenous shocks, and c is a guess for consumption at z‚Çú implied by   the conditional expectation in the Euler equation when calculated with a quadrature rule.   Note that c can be either the consumption level or some transformation (e.g. log consumption),   but the user should be consistent in the definition of the c‚Çú function with the guess c,   i.e. both should return the same transformation of consumption (e.g. both should return the level).\nùîº_quadrature::Function: a quadrature rule whose single input is a Function with a single   input, which is a shock Œµ‚Çú‚Çä‚ÇÅ.\nz‚Çú::AbstractVector: a state at which to evaluate the Euler equation error\nshock_matrix::Abstractmatrix: a NŒµ √ó T matrix of shocks drawn from the distribution of exogenous shocks.\n\nKeywords\n\nc_init::Number: an initial guess to be used when solving the \"true\" consumption policy using   quadrature. The default is the consumption policy according to the RiskAdjustedLinearization\nsummary_statistic::Function: a Function used to compute a summary statistic from the   ergodic set of Euler equation errors. The default is the maximum absolute error.\nburnin::Int: number of periods to drop as burn-in\nreturn_soln::Bool: if true, return the solution to the nonlinear equation isntead of the error\nkwargs: Any keyword arguments for nlsolve can be passed, too, e.g. ftol or autodiff   since nlsolve is used to calculate the \"true\" consumption policy.\n\n\n\n\n\n","category":"function"},{"location":"diagnostics/#RiskAdjustedLinearizations.dynamic_euler_equation_error","page":"Diagnostics","title":"RiskAdjustedLinearizations.dynamic_euler_equation_error","text":"dynamic_euler_equation_error(m, c‚Çú, logSDFxR, ùîº_quadrature, endo_states, n_aug,\n    shock_matrix, z‚Çú = m.z; c_init = NaN, summary_statistic = x -> norm(x, Inf),\n    burnin = 0, return_soln = false, kwargs...)\n\ncalculates dynamic Euler equation errors, as proposed in Den Haan (2009). The Euler equation is\n\nmath beginaligned 0 = log mathbbE_t exp(m_t + 1 + r_t + 1) = log mathbbE_tM_t + 1 R_t + 1 endaligned\n\nwhere m_t + 1 = log(M_t + 1) is the log stochastic discount factor and r_t + 1 = log(R_t + 1) is the risk free rate.\n\nThe dynamic errors are computed according the following algorithm.\n\nSimulate according to the risk-adjusted linearization time series for the state variables\nUsing the time series from 1, compute time series for consumption and  some state variable (usually capital) that can ensure budget constraints hold and markets  clear when computing consumption by applying quadrature.\nGenerate a second \"implied\" time series for consumption and the \"capital\" state variable,  starting from the same initial state as 2. Repeat the following steps at each time period.  (i)  Compute the conditional expectation in the Euler equation using quadrature to       obtain implied consumption.  (ii) Use budget constraint/market-clearing to compute implied capital.\n\nBy default, dynamic_euler_equation_error returns some summary statistic of the errors specified by the keyword summary_statistic.\n\nInputs\n\nm::RiskAdjustedLinearization: A solved instance of a risk-adjusted linearization\nc‚Çú::Function: a function of (m, z‚Çú) that calculates consumption at state z‚Çú, given the   state-space representation implied by m.\nlogSDFxR::Function: a Function evaluating m_t + 1 + r_t + 1. The Function must   take as input (m, z‚Çú, Œµ‚Çú‚Çä‚ÇÅ, c‚Çú), where m is a RiskAdjustedLinearization,   z‚Çú is a state vector at which to evaluate, Œµ‚Çú‚Çä‚ÇÅ is a draw from the distribution   of exogenous shocks, and c‚Çú is the a guess for consumption at z‚Çú implied by   the conditional expectation in the Euler equation when calculated with a quadrature rule.\nùîº_quadrature::Function: a quadrature rule whose single input is a Function with a single   input, which is a shock Œµ‚Çú‚Çä‚ÇÅ.\nendo_states::Function: augments the state variables in the risk-adjusted linearization,   usually with one additional variable, which represents capital or assets.\nn_aug::Int: number of extra state variables added by endo_states (usually 1).\nz‚Çú::AbstractVector: a state at which to evaluate the Euler equation error\nshock_matrix::Abstractmatrix: a NŒµ √ó T matrix of shocks drawn from the distribution of exogenous shocks.\n\nKeywords\n\nc_init::Number: an initial guess to be used when solving the true consumption policy using   quadrature. The default is the consumption policy according to the RiskAdjustedLinearization\nsummary_statistic::Function: a Function used to compute a summary statistic from the   ergodic set of Euler equation errors. The default is the maximum absolute error.\nburnin::Int: number of periods to drop as burn-in\nkwargs: Any keyword arguments for nlsolve can be passed, too, e.g. ftol or autodiff   since nlsolve is used to calculate the \"true\" consumption policy.\n\n\n\n\n\n","category":"function"},{"location":"diagnostics/#","page":"Diagnostics","title":"Diagnostics","text":"To make running diagnostics even easier, we also provide user-friendly functions for calculating Gauss-Hermite quadrature when shocks are Gaussian. Extensions of Gauss-Hermite quadrature rules for non-Gaussian shocks (e.g. Poisson disaster risk) should be straightforward to implement by mirroring the implementation in RiskAdjustedLinearizations.jl.","category":"page"},{"location":"diagnostics/#","page":"Diagnostics","title":"Diagnostics","text":"RiskAdjustedLinearizations.gausshermite_expectation","category":"page"},{"location":"diagnostics/#RiskAdjustedLinearizations.gausshermite_expectation","page":"Diagnostics","title":"RiskAdjustedLinearizations.gausshermite_expectation","text":"gausshermite_expectation(f, Œº, œÉ, n = 10)\ngausshermite_expectation(f, Œº, Œ£, n = 10)\ngausshermite_expectation(f, Œº, Œ£, ns)\n\ncalculates the expectation of a function of a Gaussian random variable/vector. The first method evalulates mathbbEf(X) where X sim N(mu sigma), while the other two methods evaluate mathbbEf(X) where X sim mathcalN(mu Sigma) and Sigma is diagonal. The latter two methods differ in that the first assumes the same number of quadrature points in every dimension while the second does not.\n\nInputs\n\nf::Function: some function of a random variable. If f(x) = x, then   gausshermite_expectation(f, Œº, œÉ) calculates the mean of N(mu sigma)   using 10-point Gauss-Hermite quadrature.\nŒº::Number or Œº::AbstractVector: mean of the Gaussian random variable/vector.\nœÉ::Number: standard deviation of the Gaussian random variable.\nŒ£::AbstractVector: diagonal of the variance-covariance matrix of    the Gaussian random vector.\nn::Int: number of quadrature points to use\nns::AbstractVector{Int} or ns::NTuple{N, Int} where N: number of quadrature points to use   in each dimension of the Gaussian random vector.\n\n\n\n\n\n","category":"function"},{"location":"sparse_arrays_jacs/#sparse-arrays-jacs-1","page":"Sparse Arrays and Jacobians","title":"Sparse Arrays and Jacobians","text":"","category":"section"},{"location":"sparse_arrays_jacs/#","page":"Sparse Arrays and Jacobians","title":"Sparse Arrays and Jacobians","text":"The risk-adjusted linearization of many economic models contains substantial amounts of sparsity. The matrices Gamma_5 and Gamma_6 as well as the output of the functions Lambda(cdot) and Sigma(cdot) are typically sparse. All of the Jacobians, Gamma_1, Gamma_2, Gamma_3, Gamma_4, and JmathcalV, are also very sparse. To optimize performance, RiskAdjustedLinearizations.jl allows users to leverage the sparsity of these objects. The caches for the first set of objects can be sparse matrices, assuming that Lambda(cdot) and Sigma(cdot) are written properly. The second set of objects are usually computed with forward-mode automatic differentiation. By using matrix coloring techniques implemented by SparseDiffTools, we can accelerate the calculation of these Jacobians and cache their output as sparse matrices.","category":"page"},{"location":"sparse_arrays_jacs/#","page":"Sparse Arrays and Jacobians","title":"Sparse Arrays and Jacobians","text":"These methods can be easily used through keyword arguments of the main constructor of the RiskAdjustedLinearization type. We have also written examples which show how to use these methods and time their speed. See the folder examples/sparse_methods. The script sparse_arrays_and_jacobians.jl illustrates how to apply the methods described in this documentation page while sparse_nlsolve_jacobians.jl describe how to use sparse automatic differentiation to accelerate the calculation of Jacobians during calls to nlsolve. See Numerical Algorithms for more details on the latter. Finally, the script combined_sparse_methods.jl combines these methods to achieve the fastest possible speeds with this package.","category":"page"},{"location":"sparse_arrays_jacs/#Sparsity-with-\\Gamma_5,-\\Gamma_6,-\\Lambda,-and-\\Sigma-1","page":"Sparse Arrays and Jacobians","title":"Sparsity with Gamma_5, Gamma_6, Lambda, and Sigma","text":"","category":"section"},{"location":"sparse_arrays_jacs/#","page":"Sparse Arrays and Jacobians","title":"Sparse Arrays and Jacobians","text":"The matrices Gamma_5 and Gamma_6 are constants and can be passed in directly as sparse matrices. The caches for Lambda and Sigma can be initialized as sparse matrices by using the Œõ_cache_init and Œ£_cache_init keywords for RiskAdjustedLinearization. This keyword is a function which takes as input a Tuple of Int dimensions and allocates an array with those dimensions. By default, these keyword have the values","category":"page"},{"location":"sparse_arrays_jacs/#","page":"Sparse Arrays and Jacobians","title":"Sparse Arrays and Jacobians","text":"Œõ_cache_init = dims -> Matrix{Float64}(undef, dims...)\nŒ£_cache_init = dims -> Matrix{Float64}(undef, dims...)","category":"page"},{"location":"sparse_arrays_jacs/#","page":"Sparse Arrays and Jacobians","title":"Sparse Arrays and Jacobians","text":"To use SparseMatrixCSC arrays, the user would instead pass","category":"page"},{"location":"sparse_arrays_jacs/#","page":"Sparse Arrays and Jacobians","title":"Sparse Arrays and Jacobians","text":"Œõ_cache_init = dims -> spzeros(dims...)\nŒ£_cache_init = dims -> spzeros(dims...)","category":"page"},{"location":"sparse_arrays_jacs/#","page":"Sparse Arrays and Jacobians","title":"Sparse Arrays and Jacobians","text":"However, the user should be aware of two caveats.","category":"page"},{"location":"sparse_arrays_jacs/#","page":"Sparse Arrays and Jacobians","title":"Sparse Arrays and Jacobians","text":"Using sparse arrays for caches may not always be faster","category":"page"},{"location":"sparse_arrays_jacs/#","page":"Sparse Arrays and Jacobians","title":"Sparse Arrays and Jacobians","text":"for calculating the steady state. To obtain Psi, we need to apply the Schur decomposition, which requires dense matrices. Thus, we still have to allocate dense versions of the sparse caches.","category":"page"},{"location":"sparse_arrays_jacs/#","page":"Sparse Arrays and Jacobians","title":"Sparse Arrays and Jacobians","text":"If Lambda is nonzero, then the cache for Sigma cannot be sparse.","category":"page"},{"location":"sparse_arrays_jacs/#","page":"Sparse Arrays and Jacobians","title":"Sparse Arrays and Jacobians","text":"The reason is that we need to compute (I - Œõ * Œ®) \\ Œ£, but this calculation will fail when the Œ£ is sparse. The cache, however, can be other special matrix types as long as the left division works. For example, the matrix could be a Diagonal or BandedMatrix.","category":"page"},{"location":"sparse_arrays_jacs/#Sparse-Jacobians-and-Automatic-Differentiation-1","page":"Sparse Arrays and Jacobians","title":"Sparse Jacobians and Automatic Differentiation","text":"","category":"section"},{"location":"sparse_arrays_jacs/#","page":"Sparse Arrays and Jacobians","title":"Sparse Arrays and Jacobians","text":"To calculate a risk-adjusted linearization, we need to compute the Jacobians of mu and xi with respect to z and y as well as the Jacobian of mathcalV with respect to z. These Jacobians are typically sparse because each equation in economic models only has a small subset of variables. To exploit this sparsity, we utilize methods from SparseDiffTools.jl.","category":"page"},{"location":"sparse_arrays_jacs/#","page":"Sparse Arrays and Jacobians","title":"Sparse Arrays and Jacobians","text":"There are two ways to instruct a RiskAdjustedLinearization that the Jacobians of mu, xi, and/or mathcalV are sparse. The first applies during the construction of an instance while the second occurs after an instance exists.","category":"page"},{"location":"sparse_arrays_jacs/#","page":"Sparse Arrays and Jacobians","title":"Sparse Arrays and Jacobians","text":"Note that sparse differentiation for this package is still a work in progress. While working examples exist, the code still has bugs. The major problems are listed below:","category":"page"},{"location":"sparse_arrays_jacs/#","page":"Sparse Arrays and Jacobians","title":"Sparse Arrays and Jacobians","text":"Homotopy does not work yet with sparse automatic differentiation.\nNaNs or undefined values sometimes occur during calls to nlsolve within solve!. However,  the numerical algorithm can succeed if solve! is repeatedly run, even when using the same initial guess for the coefficients (z y Psi). This happens at a sufficiently high frequency that using sparse automatic differentiation is not reliable.","category":"page"},{"location":"sparse_arrays_jacs/#Specify-Sparsity-during-Construction-1","page":"Sparse Arrays and Jacobians","title":"Specify Sparsity during Construction","text":"","category":"section"},{"location":"sparse_arrays_jacs/#","page":"Sparse Arrays and Jacobians","title":"Sparse Arrays and Jacobians","text":"When constructing a RiskAdjustedLinearization, the keyword sparse_jacobian::Vector{Symbol} is a vector containing the symbols :Œº, :Œæ, and/or :ùí±. For example, if","category":"page"},{"location":"sparse_arrays_jacs/#","page":"Sparse Arrays and Jacobians","title":"Sparse Arrays and Jacobians","text":"sparse_jacobian = [:Œº, :ùí±]","category":"page"},{"location":"sparse_arrays_jacs/#","page":"Sparse Arrays and Jacobians","title":"Sparse Arrays and Jacobians","text":"then the constructor will interpret that mu has sparse Jacobians with respect to z and y, and that mathcalV has a sparse Jacobian with respect to z.","category":"page"},{"location":"sparse_arrays_jacs/#","page":"Sparse Arrays and Jacobians","title":"Sparse Arrays and Jacobians","text":"To implement sparse differentiation, the user needs to provide a sparsity pattern and a matrix coloring vector. The user can use the keywords sparsity and colorvec to provide this information. These keywords are dictionaries whose keys are the names of the Jacobians and values are the sparsity pattern and matrix coloring vector. The relevant keys are :Œºz, :Œºy, :Œæz, :Œæy, and :Jùí±, where","category":"page"},{"location":"sparse_arrays_jacs/#","page":"Sparse Arrays and Jacobians","title":"Sparse Arrays and Jacobians","text":":Œºz and :Œºy are the Jacobians of Œº with respect to z and y,\n:Œæz and :Œæy are the Jacobians of Œæ with respect to z and y, and\n:Jùí± is the Jacobian of ùí± with respect to z.","category":"page"},{"location":"sparse_arrays_jacs/#","page":"Sparse Arrays and Jacobians","title":"Sparse Arrays and Jacobians","text":"If sparse_jacobian is nonempty, but one of these dictionaries is empty or does not contain the correct subset of the keys :Œºz, :Œºy, :Œæz, :Œæy, and :Jùí±, then we attempt to determine the sparsity pattern and/or matrix coloring vector. Once the sparsity pattern is known, the matrix coloring vector is determined by calling matrix_colors. We implement two approaches to discern the sparsity pattern. By default, we compute the dense Jacobian once using ForwardDiff and assume that any zeros in the computed Jacobian are supposed to be zero. If this assumption is true, then this Jacobian can be used as the sparsity pattern. Alternatively, the user can set the keyword sparsity_detection = true, in which case we call jacobian_sparsity from SparsityDetection.jl. to determine the sparsity pattern. Currently, only the first approach works.","category":"page"},{"location":"sparse_arrays_jacs/#","page":"Sparse Arrays and Jacobians","title":"Sparse Arrays and Jacobians","text":"For mu and xi, the first approach typically works fine. For mathcalV, however, if the user guesses that Psi is a matrix of zeros, then the Jacobian will be zero as well. A good guess of Psi is crucial to inferring the correct sparsity pattern of mathcalV because different Psi can imply different sparsity patterns. For this reason, to fully exploit the sparsity in a model, we recommend calculating the risk-adjusted linearization once using dense Jacobian methods. The calculated Jacobians can be used subsequently as the sparsity patterns.","category":"page"},{"location":"sparse_arrays_jacs/#","page":"Sparse Arrays and Jacobians","title":"Sparse Arrays and Jacobians","text":"For reference, see the docstring for RiskAdjustedLinearization.","category":"page"},{"location":"sparse_arrays_jacs/#Update-a-RiskAdjustedLinearization-with-Sparse-Jacobians-after-Construction-1","page":"Sparse Arrays and Jacobians","title":"Update a RiskAdjustedLinearization with Sparse Jacobians after Construction","text":"","category":"section"},{"location":"sparse_arrays_jacs/#","page":"Sparse Arrays and Jacobians","title":"Sparse Arrays and Jacobians","text":"Sparse Jacobians can be specified after a RiskAdjustedLinearization object m already exists by calling update_sparsity_pattern!(m, function_names). The syntax of update_sparsity_pattern! is very similar to the specification of sparse Jacobians in the constructor. The second input function_names is either a Symbol or Vector{Symbol}, and it specifies the Jacobian(s) whose sparsity pattern(s) should be updated. The relevent symbols are :Œºz, :Œºy, :Œæz, :Œæy, and :Jùí±. If the Jacobians calculated by m are dense Jacobians, then update_sparsity_pattern! will replace the functions computing dense Jacobians with functions that exploit sparsity. If the Jacobians are already being calculated as sparse Jacobians, then update_sparsity_pattern! can update the sparsity pattern and matrix coloring vector being used.","category":"page"},{"location":"sparse_arrays_jacs/#","page":"Sparse Arrays and Jacobians","title":"Sparse Arrays and Jacobians","text":"If no keywords are passed, then update_sparsity_pattern! will use the same methods as the constructor to infer the sparsity pattern. Either we compute the dense Jacobian once using ForwardDiff, or we utilize SparsityDetection. The new sparsity pattern and matrix coloring vectors can be specified using the sparsity and colorvec keywords, just like the constructor. Different values for z, y, and Psi can also be used when trying to infer the sparsity pattern by passing the new values as keywords.","category":"page"},{"location":"sparse_arrays_jacs/#","page":"Sparse Arrays and Jacobians","title":"Sparse Arrays and Jacobians","text":"RiskAdjustedLinearizations.update_sparsity_pattern!","category":"page"},{"location":"sparse_arrays_jacs/#RiskAdjustedLinearizations.update_sparsity_pattern!","page":"Sparse Arrays and Jacobians","title":"RiskAdjustedLinearizations.update_sparsity_pattern!","text":"update_sparsity_pattern!(m::RiskAdjustedLinearization, function_name::Union{Symbol, Vector{Symbol}};\n                         z::AbstractVector{<: Number} = m.z,\n                         y::AbstractVector{<: Number} = m.y,\n                         Œ®::AbstractVector{<: Number} = m.Œ®,\n                         sparsity::AbstractDict{Symbol, AbstractMatrix} = Dict{Symbol, AbstractMatrix}(),\n                         colorvec::AbstractDict{Symbol, <: AbstractVector{Int}} = Dict{Symbol, Vector{Int}}(),\n                         sparsity_detection::Bool = false)\n\nupdates the Jacobians of Œº, Œæ, and/or ùí± in m with a new sparsity pattern. The Jacobians to be updated are specified by function_name, e.g. function_name = [:Œº, :Œæ, :ùí±].\n\nIf the keyword sparsity is empty, then the function attempts to determine the new sparsity pattern by computing the Jacobian via automatic differentiation and assuming any zeros are supposed to be zero. Keywords provide guesses for the coefficients (z y Psi) that are required to calculate the Jacobians.\n\nKeywords\n\nz: state coefficients at steady state\ny: jump coefficients at steady state\nŒ®: coefficients for mapping from states to jumps\nsparsity: key-value pairs can be used to specify new sparsity patterns for the Jacobian functions   Œºz, Œºy, Œæz, Œæy, and Jùí±.\ncolorvec: key-value pairs can be used to specify new matrix coloring vectors for the Jacobian functions   Œºz, Œºy, Œæz, Œæy, and Jùí±.\nsparsity_detection: use SparsityDetection.jl to determine the sparsity pattern.\n\n\n\n\n\n","category":"function"},{"location":"risk_adjusted_linearization/#risk-adjusted-linearization-1","page":"Risk-Adjusted Linearizations","title":"Risk-Adjusted Linearizations","text":"","category":"section"},{"location":"risk_adjusted_linearization/#Theory-1","page":"Risk-Adjusted Linearizations","title":"Theory","text":"","category":"section"},{"location":"risk_adjusted_linearization/#Nonlinear-Model-1","page":"Risk-Adjusted Linearizations","title":"Nonlinear Model","text":"","category":"section"},{"location":"risk_adjusted_linearization/#","page":"Risk-Adjusted Linearizations","title":"Risk-Adjusted Linearizations","text":"Most dynamic economic models can be formulated as the system of nonlinear equations","category":"page"},{"location":"risk_adjusted_linearization/#","page":"Risk-Adjusted Linearizations","title":"Risk-Adjusted Linearizations","text":"beginaligned\n    z_t + 1  = mu(z_t y_t) + Lambda(z_t)(y_t + 1 - mathbbE_t y_t + 1) + Sigma(z_t) varepsilon_t + 1\n    0  = logmathbbE_texp(xi(z_t y_t) + Gamma_5 z_t + 1 + Gamma_6 y_t + 1)\nendaligned","category":"page"},{"location":"risk_adjusted_linearization/#","page":"Risk-Adjusted Linearizations","title":"Risk-Adjusted Linearizations","text":"The vectors z_tin mathbbR^n_z and y_t in mathbbR^n_y are the state and jump variables, respectively. The first vector equation comprise the transition equations of the state variables. The second vector equation comprise the model's expectational equations, which are typically the first-order conditions for the jump variables from agents' optimization problem. The exogenous shocks varepsilon_t inmathbbR^n_varepsilon form a martingale difference sequence and can therefore be non-Gaussian. Given some differentiable mapping alphamathbbR^n_zrightarrowmathbbR^n_varepsilon, the random variable X_t = alpha(z_t)^T varepsilon_t + 1 has the differentiable, conditional (on z_t) cumulant generating function","category":"page"},{"location":"risk_adjusted_linearization/#","page":"Risk-Adjusted Linearizations","title":"Risk-Adjusted Linearizations","text":"beginaligned\nccgfalpha(z_t) mid z_t = logmathbbE_texp(alpha(z_t)^T varepsilon_t + 1)\nendaligned","category":"page"},{"location":"risk_adjusted_linearization/#","page":"Risk-Adjusted Linearizations","title":"Risk-Adjusted Linearizations","text":"The functions","category":"page"},{"location":"risk_adjusted_linearization/#","page":"Risk-Adjusted Linearizations","title":"Risk-Adjusted Linearizations","text":"beginaligned\nximathbbR^2n_y + 2n_zrightarrow mathbbR^n_y quad mumathbbR^n_y + n_zrightarrow mathbbR^n_z\nLambdamathbbR^n_z rightarrow mathbbR^n_z times n_y  quad SigmamathbbR^n_z rightarrow mathbbR^n_ztimes n_varepsilon\nendaligned","category":"page"},{"location":"risk_adjusted_linearization/#","page":"Risk-Adjusted Linearizations","title":"Risk-Adjusted Linearizations","text":"are differentiable. The first two functions characterize the effects of time t variables on the expectational and state transition equations. The function Lambda characterizes heteroskedastic endogenous risk that depends on innovations in jump variables while the function Sigma characterizes exogenous risk. Note that Sigma is not the variance-covariance matrix of varepsilon_t. The functions Lambda and Sigma can also depend on jump variables. Denote the jump-dependent versions as tildeLambdamathbbR^n_ztimes n_y rightarrow mathbbR^n_z times n_y and tildeSigmamathbbR^n_z times n_y rightarrow mathbbR^n_ztimes n_varepsilon. If there exists a mapping y_t = y(z_t), then we define Lambda(z_t) = tildeLambda(z_t y(z_t)) and Sigma(z_t) = tildeSigma(z_t y(z_t)).","category":"page"},{"location":"risk_adjusted_linearization/#","page":"Risk-Adjusted Linearizations","title":"Risk-Adjusted Linearizations","text":"The expectational equations can be simplified as","category":"page"},{"location":"risk_adjusted_linearization/#","page":"Risk-Adjusted Linearizations","title":"Risk-Adjusted Linearizations","text":"beginaligned\n0  = logmathbbE_texp(xi(z_t y_t) + Gamma_5 z_t + 1 + Gamma_6 y_t + 1)\n   = logexp(xi(z_t y_t))mathbbE_texp(Gamma_5 z_t + 1 + Gamma_6 y_t + 1)\n   = xi(z_t y_t) + Gamma_5mathbbE_t z_t + 1 + Gamma_6 mathbbE_t y_t + 1 + logmathbbE_texp(Gamma_5 z_t + 1 + Gamma_6 y_t + 1) - (Gamma_5mathbbE_t z_t + 1 + Gamma_6 mathbbE_t y_t + 1)\n   = xi(z_t y_t) + Gamma_5mathbbE_t z_t + 1 + Gamma_6 mathbbE_t y_t + 1 + mathcalV(Gamma_5 z_t + 1 + Gamma_6 y_t + 1)\nendaligned","category":"page"},{"location":"risk_adjusted_linearization/#","page":"Risk-Adjusted Linearizations","title":"Risk-Adjusted Linearizations","text":"where the last term is defined to be","category":"page"},{"location":"risk_adjusted_linearization/#","page":"Risk-Adjusted Linearizations","title":"Risk-Adjusted Linearizations","text":"beginaligned\nmathcalV(x_t + 1) = logmathbbE_texp(x_t + 1) - mathbbE_t x_t + 1\nendaligned","category":"page"},{"location":"risk_adjusted_linearization/#","page":"Risk-Adjusted Linearizations","title":"Risk-Adjusted Linearizations","text":"As Lopez et al. (2018) describe it, this quantity \"is a relative entropy measure, i.e. a nonnegative measure of dispersion that generalizes variance.\"","category":"page"},{"location":"risk_adjusted_linearization/#affine-theory-1","page":"Risk-Adjusted Linearizations","title":"Risk-Adjusted Linearizations by Affine Approximation","text":"","category":"section"},{"location":"risk_adjusted_linearization/#","page":"Risk-Adjusted Linearizations","title":"Risk-Adjusted Linearizations","text":"Many economic models are typically solved by perturbation around the deterministic steady state. To break certainty equivalence so that asset pricing is meaningful, these perturbations need to be at least third order. However, even third-order perturbations can poorly approximate the true global solution. A key problem is that the economy may not spend much time near the deterministic steady state, so a perturbation around this point will be inaccurate.","category":"page"},{"location":"risk_adjusted_linearization/#","page":"Risk-Adjusted Linearizations","title":"Risk-Adjusted Linearizations","text":"Instead of perturbing the model's nonlinear equations around the deterministic steady state, we could perturb around the stochastic or \"risky\" steady state. This point is better for a perturbation because the economy will spend a large amount of time near the stochastic steady state. Lopez et al. (2018) show that an affine approximation of the model's nonlinear equation is equivalent to a linearization around the stochastic steady state. Further, they confirm that in practice this \"risk-adjusted\" linearization approximates global solutions of canonical economic models very well and outperforms perturbations around the deterministic steady state.","category":"page"},{"location":"risk_adjusted_linearization/#","page":"Risk-Adjusted Linearizations","title":"Risk-Adjusted Linearizations","text":"The affine approximation of an dynamic economic model is","category":"page"},{"location":"risk_adjusted_linearization/#","page":"Risk-Adjusted Linearizations","title":"Risk-Adjusted Linearizations","text":"beginaligned\n    mathbbEz_t + 1  = mu(z y) + Gamma_1(z_t - z) + Gamma_2(y_t - y)\n    0                       = xi(z y) + Gamma_3(z_t - z) + Gamma_4(y_t - y) + Gamma_5 mathbbE_t z_t + 1 + Gamma_6 mathbbE_t y_t + 1 + mathcalV(z) + JmathcalV(z)(z_t  - z)\nendaligned","category":"page"},{"location":"risk_adjusted_linearization/#","page":"Risk-Adjusted Linearizations","title":"Risk-Adjusted Linearizations","text":"where Gamma_1 Gamma_2 are the Jacobians of mu with respect to z_t and y_t, respectively; Gamma_3 Gamma_4 are the Jacobians of xi with respect to z_t and y_t, respectively; Gamma_5 Gamma_6 are constant matrices; mathcalV(z) is the model's entropy; and JmathcalV(z) is the Jacobian of the entropy;","category":"page"},{"location":"risk_adjusted_linearization/#","page":"Risk-Adjusted Linearizations","title":"Risk-Adjusted Linearizations","text":"and the state variables z_t and jump variables y_t follow","category":"page"},{"location":"risk_adjusted_linearization/#","page":"Risk-Adjusted Linearizations","title":"Risk-Adjusted Linearizations","text":"beginaligned\n    z_t + 1  = z + Gamma_1(z_t - z) + Gamma_2(y_t - y) + (I_n_z - Lambda(z_t) Psi)^-1Sigma(z_t)varepsilon_t + 1\n    y_t        = y + Psi(z_t - z)\nendaligned","category":"page"},{"location":"risk_adjusted_linearization/#","page":"Risk-Adjusted Linearizations","title":"Risk-Adjusted Linearizations","text":"The unknowns (z y Psi) solve the system of equations","category":"page"},{"location":"risk_adjusted_linearization/#","page":"Risk-Adjusted Linearizations","title":"Risk-Adjusted Linearizations","text":"beginaligned\n0  = mu(z y) - z\n0  = xi(z y) + Gamma_5 z + Gamma_6 y + mathcalV(z)\n0  = Gamma_3 + Gamma_4 Psi + (Gamma_5 + Gamma_6 Psi)(Gamma_1 + Gamma_2 Psi) + JmathcalV(z)\nendaligned","category":"page"},{"location":"risk_adjusted_linearization/#","page":"Risk-Adjusted Linearizations","title":"Risk-Adjusted Linearizations","text":"Under an affine approximation, the entropy term is a nonnegative function mathcalVmathbbR^n_z rightarrow mathbbR_+^n_y defined such that","category":"page"},{"location":"risk_adjusted_linearization/#","page":"Risk-Adjusted Linearizations","title":"Risk-Adjusted Linearizations","text":"beginaligned\nmathcalV(z_t) equiv mathcalV_t(exp((Gamma_5 + Gamma_6 Psi)z_t + 1)) = vecccgf(Gamma_5 + Gamma_6 Psi)(I_n_z - Lambda(z_t) Psi)^-1 Sigma(z_t) mid z_t\nendaligned","category":"page"},{"location":"risk_adjusted_linearization/#","page":"Risk-Adjusted Linearizations","title":"Risk-Adjusted Linearizations","text":"where the notation vecccgf means that each component ccgf_icdot mid cdot is a conditional cumulant-generating function. Explicitly, define","category":"page"},{"location":"risk_adjusted_linearization/#","page":"Risk-Adjusted Linearizations","title":"Risk-Adjusted Linearizations","text":"beginaligned\nA(z_t) = (Gamma_5 + Gamma_6 Psi)(I_n_z - Lambda(z_t) Psi)^-1 Sigma(z_t) = A_1(z_t) dots A_n_y(z_t)^T\nendaligned","category":"page"},{"location":"risk_adjusted_linearization/#","page":"Risk-Adjusted Linearizations","title":"Risk-Adjusted Linearizations","text":"Each A_i(z_t) is a mapping from z_t to the ith row vector in A(z_t). Then ccgf_icdot mid cdot is","category":"page"},{"location":"risk_adjusted_linearization/#","page":"Risk-Adjusted Linearizations","title":"Risk-Adjusted Linearizations","text":"beginaligned\nccgf_iA_i(z_t)mid z_t = logmathbbE_texp(A_i(z_t) varepsilon_t + 1)\nendaligned","category":"page"},{"location":"risk_adjusted_linearization/#","page":"Risk-Adjusted Linearizations","title":"Risk-Adjusted Linearizations","text":"Every ccgf_icdot mid cdot corresponds to an expectational equation and thus acts as a risk correction to each one. In the common case where the individual components of varepsilon_t + 1 are independent, ccgf_i simplifies to","category":"page"},{"location":"risk_adjusted_linearization/#","page":"Risk-Adjusted Linearizations","title":"Risk-Adjusted Linearizations","text":"beginaligned\nccgf_iA_i(z_t)mid z_t = sum_j = 1^n_varepsilonlogmathbbE_texp(A_ij(z_t) varepsilon_j t + 1)\nendaligned","category":"page"},{"location":"risk_adjusted_linearization/#","page":"Risk-Adjusted Linearizations","title":"Risk-Adjusted Linearizations","text":"i.e. it is the sum of the cumulant-generating functions for each shock varepsilon_j t + 1.","category":"page"},{"location":"risk_adjusted_linearization/#","page":"Risk-Adjusted Linearizations","title":"Risk-Adjusted Linearizations","text":"To see why mathcalV_t(exp((Gamma_5 + Gamma_6 Psi)z_t + 1)) can be expressed as the conditional cumulant-generating function of (Gamma_5 + Gamma_6 Psi)(I_n_z - Lambda(z_t) Psi)^-1 Sigma(z_t), observe that","category":"page"},{"location":"risk_adjusted_linearization/#","page":"Risk-Adjusted Linearizations","title":"Risk-Adjusted Linearizations","text":"beginaligned\nmathcalV_t(exp((Gamma_5 + Gamma_6 Psi)z_t + 1))  = logmathbbE_texp((Gamma_5 + Gamma_6 Psi)z_t + 1) - mathbbE_t(Gamma_5 + Gamma_6 Psi)z_t + 1\n                              = logleft(fracmathbbE_texp((Gamma_5 + Gamma_6 Psi)z_t + 1)exp(mathbbE_t(Gamma_5 + Gamma_6 Psi)z_t + 1)right)\nendaligned","category":"page"},{"location":"risk_adjusted_linearization/#","page":"Risk-Adjusted Linearizations","title":"Risk-Adjusted Linearizations","text":"Since mathbbE_t(Gamma_5 + Gamma_6 Psi)z_t + 1 is a conditional expectation, it is measurable with respect to the time-t information set. Therefore, we can move the denominator of the fraction within the logarithm inside the numerator's conditional expectation.","category":"page"},{"location":"risk_adjusted_linearization/#","page":"Risk-Adjusted Linearizations","title":"Risk-Adjusted Linearizations","text":"beginaligned\nmathcalV_t(exp((Gamma_5 + Gamma_6 Psi)z_t + 1))  = logmathbbE_tleftexpleft((Gamma_5 + Gamma_6 Psi)z_t + 1 - mathbbE_t(Gamma_5 + Gamma_6 Psi)z_t + 1right)right\n = logmathbbE_tleftexpleft((Gamma_5 + Gamma_6 Psi)(z_t + 1 - mathbbE_tz_t + 1)right)right\nendaligned","category":"page"},{"location":"risk_adjusted_linearization/#","page":"Risk-Adjusted Linearizations","title":"Risk-Adjusted Linearizations","text":"Using the postulated law of motion for states,","category":"page"},{"location":"risk_adjusted_linearization/#","page":"Risk-Adjusted Linearizations","title":"Risk-Adjusted Linearizations","text":"beginaligned\nz_t + 1 - mathbbE_tz_t + 1  = mu(z_t y_t) + Lambda(z_t)(y_t + 1 - mathbbE_ty_t + 1) + Sigma(z_t) varepsilon_t + 1 - mu(z_t y_t)\n =  Lambda(z_t) Psi  (z_t + 1 - mathbbE_tz_t + 1) + Sigma(z_t) varepsilon_t + 1\n(I - Lambda(z_t) Psi) (z_t + 1 - mathbbE_tz_t + 1)   = Sigma(z_t) varepsilon_t + 1\nendaligned","category":"page"},{"location":"risk_adjusted_linearization/#","page":"Risk-Adjusted Linearizations","title":"Risk-Adjusted Linearizations","text":"Therefore, the entropy term mathcalV_t(cdot) becomes","category":"page"},{"location":"risk_adjusted_linearization/#","page":"Risk-Adjusted Linearizations","title":"Risk-Adjusted Linearizations","text":"beginaligned\nmathcalV_t(exp((Gamma_5 + Gamma_6 Psi)z_t + 1))  = logmathbbE_tleftexpleft((Gamma_5 + Gamma_6 Psi)(I - Lambda(z_t) Psi)^-1 Sigma(z_t) varepsilon_t + 1right)right\nendaligned","category":"page"},{"location":"risk_adjusted_linearization/#","page":"Risk-Adjusted Linearizations","title":"Risk-Adjusted Linearizations","text":"The RHS is a vector of logarithms of expected values of linear combinations of the shocks varepsilon_t + 1 with coefficients given by the rows of (Gamma_5 + Gamma_6 Psi)(I - Lambda(z_t) Psi)^-1 Sigma(z_t). Thus, each element of mathcalV_t(cdot) is the conditional cumulant-generating function of the random vector varepsilon_t + 1 evaluated at one of the rows of (Gamma_5 + Gamma_6 Psi)(I - Lambda(z_t) Psi)^-1 Sigma(z_t), as claimed.","category":"page"},{"location":"risk_adjusted_linearization/#","page":"Risk-Adjusted Linearizations","title":"Risk-Adjusted Linearizations","text":"Refer to Lopez et al. (2018) \"Risk-Adjusted Linearizations of Dynamic Equilibrium Models\" for more details about the theory justifying this approximation approach. See Deriving the conditional cumulant generating function for some guidance on calculating the ccgf, which many users may not have seen before.","category":"page"},{"location":"risk_adjusted_linearization/#implement-ral-1","page":"Risk-Adjusted Linearizations","title":"Implementation as RiskAdjustedLinearization","text":"","category":"section"},{"location":"risk_adjusted_linearization/#","page":"Risk-Adjusted Linearizations","title":"Risk-Adjusted Linearizations","text":"We implement risk-adjusted linearizations of nonlinear dynamic economic models through the wrapper type RiskAdjustedLinearization. The user only needs to define the functions and matrices characterizing the equilibrium of the nonlinear model. Once these functions are defined, the user can create a RiskAdjustedLinearization object, which will automatically create the Jacobian functions needed to compute the affine approximation.","category":"page"},{"location":"risk_adjusted_linearization/#","page":"Risk-Adjusted Linearizations","title":"Risk-Adjusted Linearizations","text":"To ensure efficiency in speed and memory, this package takes advantage of a number of features that are easily accessible through Julia.","category":"page"},{"location":"risk_adjusted_linearization/#","page":"Risk-Adjusted Linearizations","title":"Risk-Adjusted Linearizations","text":"The Jacobians are calculated using forward-mode automatic differentiation rather than symbolic differentiation.\nThe Jacobian functions are constructed to be in-place with pre-allocated caches.\nFunctions provided by the user will be converted into in-place functions with pre-allocated caches.\nCalls to nonlinear equation solver nlsolve are accelerated by exploiting sparsity in Jacobians with SparseDiffTools.jl.\nCalculation of Jacobians of mu, xi, and mathcalV with automatic differentiation is accelerated by exploiting sparsity with SparseDiffTools.jl.","category":"page"},{"location":"risk_adjusted_linearization/#","page":"Risk-Adjusted Linearizations","title":"Risk-Adjusted Linearizations","text":"See the Example for how to use the type. To compare this package's speed with the original MATLAB code, run the wac_disaster.jl or rbc_cc.jl scripts. These scripts assess how long it takes to calculate a risk-adjusted linearization using the two numerical algorithms implemented by this package and by the original authors. The relaxation algorithm is generally around 50x-100x faster while the homotopy algorithm 3x-4x times faster.","category":"page"},{"location":"risk_adjusted_linearization/#","page":"Risk-Adjusted Linearizations","title":"Risk-Adjusted Linearizations","text":"The docstring for the constructor is given below. Some of the keywords allow the user to exploit sparsity in the objects comprising a risk-adjusted linearization. For more details on sparse methods, see Spares Arrays and Jacobians.","category":"page"},{"location":"risk_adjusted_linearization/#","page":"Risk-Adjusted Linearizations","title":"Risk-Adjusted Linearizations","text":"RiskAdjustedLinearizations.RiskAdjustedLinearization","category":"page"},{"location":"risk_adjusted_linearization/#RiskAdjustedLinearizations.RiskAdjustedLinearization","page":"Risk-Adjusted Linearizations","title":"RiskAdjustedLinearizations.RiskAdjustedLinearization","text":"RiskAdjustedLinearization(Œº, Œõ, Œ£, Œæ, Œì‚ÇÖ, Œì‚ÇÜ, ccgf, z, y, Œ®, NŒµ)\nRiskAdjustedLinearization(nonlinear_system, linearized_system, z, y, Œ®, Nz, Ny, NŒµ)\n\nCreates a first-order perturbation around the stochastic steady state of a discrete-time dynamic economic model.\n\nThe first method is the main constructor most users will want, while the second method is the default constructor.\n\nInputs for First Method\n\nŒº::Function: expected state transition function\nŒæ::Function: nonlinear terms of the expectational equations\nccgf::Function: conditional cumulant generating function of the exogenous shocks\nŒõ::Function or Œõ::AbstractMatrix: function or matrix mapping endogenous risk into state transition equations\nŒ£::Function or Œ£::AbstractMatrix: function or matrix mapping exogenous risk into state transition equations\nŒì‚ÇÖ::AbstractMatrix{<: Number}: coefficient matrix on one-period ahead expectation of state variables\nŒì‚ÇÜ::AbstractMatrix{<: Number}: coefficient matrix on one-period ahead expectation of jump variables\nz::AbstractVector{<: Number}: state variables in stochastic steady state\ny::AbstractVector{<: Number}: jump variables in stochastic steady state\nŒ®::AbstractMatrix{<: Number}: matrix linking deviations in states to deviations in jumps, i.e. y_t - y = Psi(z_t - z).\nNŒµ::Int: number of exogenous shocks\n\nKeywords for First Method\n\nsss_vector_cache_init::Function = dims -> Vector{T}(undef, dims): initializer for the cache of steady state vectors.\nŒõ_cache_init::Function = dims -> Matrix{T}(undef, dims):  initializer for the cache of Œõ\nŒ£_cache_init::Function = dims -> Matrix{T}(undef, dims):  initializer for the cache of Œõ\njacobian_cache_init::Function = dims -> Matrix{T}(undef, dims): initializer for the cache of the Jacobians of Œº, Œæ, and ùí±.\njump_dependent_shock_matrices::Bool = false: if true, Œõ and Œ£ are treated as Œõ(z, y) and Œ£(z, y)   to allow dependence on jumps.\nsparse_jacobian::Vector{Symbol} = Symbol[]: pass the symbols :Œº, :Œæ, and/or :ùí± to declare that   the Jacobians of these functions are sparse and should be differentiated using sparse methods from SparseDiffTools.jl\nsparsity::AbstractDict = Dict{Symbol, Mtarix}(): a dictionary for declaring the   sparsity patterns of the Jacobians of Œº, Œæ, and ùí±. The relevant keys are :Œºz, :Œºy, :Œæz, :Œæy, and :Jùí±.\ncolorvec::AbstractDict = Dict{Symbol, Vector{Int}}(): a dictionary for declaring the   the matrix coloring vector. The relevant keys are :Œºz, :Œºy, :Œæz, :Œæy, and :Jùí±.\nsparsity_detection::Bool = false: if true, use SparseDiffTools to determine the sparsity pattern.   When false (default), the sparsity pattern is estimated by differentiating the Jacobian once   with ForwardDiff and assuming any zeros in the calculated Jacobian are supposed to be zeros.\n\nInputs for Second Method\n\nnonlinear_system::RALNonlinearSystem\nlinearized_system::RALLinearizedSystem\nz::AbstractVector{<: Number}: state variables in stochastic steady state\ny::AbstractVector{<: Number}: jump variables in stochastic steady state\nŒ®::AbstractMatrix{<: Number}: matrix linking deviations in states to deviations in jumps, i.e. y_t - y = Psi(z_t - z).\nNz::Int: number of state variables\nNy::Int: number of jump variables\nNŒµ::Int: number of exogenous shocks\n\n\n\n\n\n","category":"type"},{"location":"risk_adjusted_linearization/#Helper-Types-1","page":"Risk-Adjusted Linearizations","title":"Helper Types","text":"","category":"section"},{"location":"risk_adjusted_linearization/#","page":"Risk-Adjusted Linearizations","title":"Risk-Adjusted Linearizations","text":"To organize the functions comprisng a risk-adjusted linearization, we create two helper types, RALNonlinearSystem and RALLinearizedSystem. The first type holds the mu, Lambda, Sigma, xi, and mathcalV functions while the second type holds the mu_z, mu_y, xi_z, xi_y, JmathcalV, Gamma_5, and Gamma_6 quantities. The RALNonlinearSystem type holds potentially nonlinear functions, and in particular mu, xi, and mathcalV, which need to be linearized (e.g. by automatic differentiation). The RALLinearizedSystem holds both matrices that are only relevant once the model is linearized, such as Gamma_1 (calculated by mu_z), as well as Gamma_5 and Gamma_6 since these latter two quantities are always constant matrices.","category":"page"},{"location":"risk_adjusted_linearization/#","page":"Risk-Adjusted Linearizations","title":"Risk-Adjusted Linearizations","text":"Aside from providing a way to organize the various functions comprising a risk-adjusted linearization, these helper types do not have much additional functionality. The update! functions for a RiskAdjustedLinearization, for example, are implemented underneath the hood by calling update! functions written for the RALNonlinearSystem and RALLinearizedSystem.","category":"page"},{"location":"caching/#caching-1","page":"Caching","title":"Caching","text":"","category":"section"},{"location":"caching/#","page":"Caching","title":"Caching","text":"If users create a RiskAdjustedLinearization with the constructor","category":"page"},{"location":"caching/#","page":"Caching","title":"Caching","text":"RiskAdjustedLinearization(Œº, Œõ, Œ£, Œæ, Œì‚ÇÖ, Œì‚ÇÜ, ccgf, z, y, Œ®, NŒµ)","category":"page"},{"location":"caching/#","page":"Caching","title":"Caching","text":"where Œº, Œõ, Œ£, Œæ, ccgf are either in-place or out-of-place functions, then we use the wrapper types RALF1 and RALF2 to convert these functions to non-allocating ones. The implementation of these wrappers is similar to the implementation of LinSolveFactorize in DifferentialEquations.jl. See Automated Caching via RALF1 and RALF2 for further details. Unlike LinSolveFactorize, however, we need to be able to automatically differentiate with RALF1 and RALF2 so the cache needs to be handled more carefully. To do this, we utilize and extend the DiffCache type implemented by DiffEqBase.jl.","category":"page"},{"location":"caching/#new-cache-types-1","page":"Caching","title":"TwoDiffCache and ThreeDiffCache","text":"","category":"section"},{"location":"caching/#","page":"Caching","title":"Caching","text":"The idea for DiffCache is that you need two caches, one for Dual numbers when applying automatic differentiation and one for the subtype of Number used for the actual array (e.g. Float64). For the Lambda and Sigma functions, this type works because they are functions of one input variables. The functions mu_z, mu_y, xi_z, and xi_y also can use DiffCache once it is extended to work for functions with two input variables (e.g. the chunk size should depend on the length of both input variables).","category":"page"},{"location":"caching/#","page":"Caching","title":"Caching","text":"However, for the mu, xi, and mathcalV functions, we actually need multiple caches for Dual numbers that differ in their chunk size. The reason is that not only do we need to be able to evaluate them with Dual numbers but we also need to apply automatic differentiation to calculate their Jacobians. Because all three of these functions take two input variables, the chunk size for the cache used to evaluate the functions themselves will be different from the cache used to calculate the Jacobians, which occur with respect to only one of the input variables.","category":"page"},{"location":"caching/#","page":"Caching","title":"Caching","text":"Note that, by default, the cache is initialized with undefined values if arrays are dense and zeros if arrays are sparse.","category":"page"},{"location":"caching/#ralfwrappers-1","page":"Caching","title":"Automated Caching via RALF1, RALF2, RALF3, and RALF4 Wrappers","text":"","category":"section"},{"location":"caching/#","page":"Caching","title":"Caching","text":"The RALF1 type applies to functions with 1 input variables (Lambda and Sigma), RALF2 to functions with 2 input variables (e.g. mu, mu_z), and so on. The way these wrappers work is that they take a user-defined function f and convert it to a new in-place function whose first input argument is a cache, which is a DiffCache, TwoDiffCache, or a ThreeDiffCache. The RALF1, RALF2, RALF3, and RALF4 types are callable in the same way LinSolveFactorize is.","category":"page"},{"location":"caching/#","page":"Caching","title":"Caching","text":"For RALF2, the syntax (x::RALF2)(x1, x2) on its own would not work, however, because (1) it is not clear which input should be used to infer whether or not to use a Dual cache and (2) there are potentially multiple Dual caches. To get around this problem, we add an optional third argument named select, which is a Tuple{Int, Int}. The first element specifies which input argument to use for infering whether a Dual cache is needed, and the second element specifies which cache to use. By default, select = (1, 1). This approach is the same for RALF3 and RALF4. The types RALF3 and RALF4 are only relevant if Lambda and Sigma depend on jump variables.","category":"page"},{"location":"#Home-1","page":"Home","title":"RiskAdjustedLinearizations.jl","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"This package provides a user-friendly API for efficiently calculating risk-adjusted linearizations of dynamic economic models. These linearizations are equivalent to first-order perturbations around the stochastic steady state and are solved by computing affine approximations.","category":"page"},{"location":"#License-1","page":"Home","title":"License","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"This content is released under the MIT License.","category":"page"},{"location":"#Contents-1","page":"Home","title":"Contents","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"Pages = [\"risk_adjusted_linearization.md\", \"sparse_arrays_jacs.md\", \"numerical_algorithms.md\", \"example.md\", \"caching.md\", \"diagnostics.md\", \"tips.md\"]","category":"page"},{"location":"numerical_algorithms/#numerical-algorithms-1","page":"Numerical Algorithms","title":"Numerical Algorithms","text":"","category":"section"},{"location":"numerical_algorithms/#","page":"Numerical Algorithms","title":"Numerical Algorithms","text":"To calculate the risk-adjusted linearization, we need to solve a system of nonlinear equations. These equations are generally solvable using Newton-type methods. The package currently has two available algorithms, relaxation and homotopy continuation","category":"page"},{"location":"numerical_algorithms/#solve!-1","page":"Numerical Algorithms","title":"solve!","text":"","category":"section"},{"location":"numerical_algorithms/#","page":"Numerical Algorithms","title":"Numerical Algorithms","text":"The primary interface for calculating a risk-adjusted linearization once a RiskAdjustedLinearization object is created is the function solve!. The user selects the desired numerical algorithm through algorithm keyword of solve!.","category":"page"},{"location":"numerical_algorithms/#","page":"Numerical Algorithms","title":"Numerical Algorithms","text":"All of the available algorithms need to solve a system of nonlinear equations. We use nlsolve for this purpose, and all keyword arguments for nlsolve can be passed as keyword arguments to solve!, e.g. autodiff and ftol. The user can also exploit sparsity in the Jacobian of the system of nonlinear equations to accelerate nlsolve by using the keywords sparse_jacobian, sparsity, colorvec, jac_cache, and/or sparsity_detection. For details, see Exploiting Sparsity.","category":"page"},{"location":"numerical_algorithms/#","page":"Numerical Algorithms","title":"Numerical Algorithms","text":"RiskAdjustedLinearizations.solve!","category":"page"},{"location":"numerical_algorithms/#RiskAdjustedLinearizations.solve!","page":"Numerical Algorithms","title":"RiskAdjustedLinearizations.solve!","text":"solve!(m; algorithm = :relaxation, autodiff = :central, verbose = :high, kwargs...)\nsolve!(m, z0, y0; kwargs...)\nsolve!(m, z0, y0, Œ®0; kwargs...)\n\ncomputes the risk-adjusted linearization of the dynamic economic model described by m and updates m with the solution, e.g. the coefficients (z y Psi).\n\nThe three available solve! algorithms are slight variations on each other.\n\nMethod 1: uses the z, y, and Œ® fields of m as initial guesses   for (z y Psi) and proceeds with the numerical algorithm   specified by algorithm\nMethod 2: uses z0 and y0 as initial guesses for the deterministic   steady state, which is then used as the initial guess for (z Y Psi)   for the numerical algorithm specified by algorithm.\nMethod 3: uses z0, y0, and Œ®0 as initial guesses for (z Y Psi)   and proceeds with the numerical algorithm specified by algorithm.\n\nInputs\n\nm::RiskAdjustedLinearization: object holding functions needed to calculate   the risk-adjusted linearization\nz0::AbstractVector{S1}: initial guess for z\ny0::AbstractVector{S1}: initial guess for y\nŒ®0::AbstractVector{S1}: initial guess for Psi\nS1 <: Real\n\nKeywords\n\nalgorithm::Symbol = :relaxation: which numerical algorithm to use? Can be one of [:relaxation, :homotopy, :deterministic]\nautodiff::Symbol = :central: use autodiff or not? This keyword is the same as in nlsolve\nuse_anderson::Bool = false: use Anderson acceleration if the relaxation algorithm is applied. Defaults to false\nstep::Float64 = .1: size of step from 0 to 1 if the homotopy algorithm is applied. Defaults to 0.1\nsparse_jacobian::Bool = false: if true, exploit sparsity in the Jacobian in calls to nlsolve using SparseDiffTools.jl.   If jac_cache and sparsity are nothing, then solve! will attempt to determine the sparsity pattern.\nsparsity::Union{AbstractArray, Nothing} = nothing: sparsity pattern for the Jacobian in calls to nlsolve\ncolorvec = nothing: matrix coloring vector for sparse Jacobian in calls to nlsolve\njac_cache = nothing: pre-allocated Jacobian cache for calls to nlsolve during the numerical algorithms\nsparsity_detection::Bool = false: If true, use SparsityDetection.jl to detect sparsity pattern (only relevant if   both jac_cache and sparsity are nothing). If false,  then the sparsity pattern is   determined by using finite differences to calculate a Jacobian and assuming any zeros will always be zero.   Currently, SparsityDetection.jl fails to work.\n\nThe solution algorithms all use nlsolve to calculate the solution to systems of nonlinear equations. The user can pass in any of the keyword arguments for nlsolve to adjust the settings of the nonlinear solver.\n\nFor the keywords relevant to specific methods, see the docstring for the underlying method being called. Note these methods are not exported.\n\n:relaxation -> relaxation!\n:homotopy -> homotopy!\n:deterministic -> deterministic_steadystate!\n\n\n\n\n\n","category":"function"},{"location":"numerical_algorithms/#relaxation-1","page":"Numerical Algorithms","title":"Relaxation","text":"","category":"section"},{"location":"numerical_algorithms/#","page":"Numerical Algorithms","title":"Numerical Algorithms","text":"The first and default numerical algorithm is a relaxation algorithm. The key problem in solving the equations characterizing (z y Psi) is that it is difficult to jointly solve the nonlinear matrix equation for Psi along with the steady-state equations for z and y due to the presence of the entropy term. The relaxation algorithm splits the solution of these equations into two steps, which allows us to calculate guesses of Psi using linear algebra. It is in this sense that this iterative algorithm is a relaxation algorithm.","category":"page"},{"location":"numerical_algorithms/#","page":"Numerical Algorithms","title":"Numerical Algorithms","text":"The system of equations characterizing the coefficients (z y Psi) are solved iteratively in two separate steps. Given previous guesses (z_n - 1 y_n - 1 Psi_n - 1), we calculate (z_n y_n) such that","category":"page"},{"location":"numerical_algorithms/#","page":"Numerical Algorithms","title":"Numerical Algorithms","text":"beginaligned\n0  = mu(z_n y_n) - z_n\n0  = xi(z_n y_n) + Gamma_5 z_n + Gamma_6 y_n + mathcalV(z_n - 1)\nendaligned","category":"page"},{"location":"numerical_algorithms/#","page":"Numerical Algorithms","title":"Numerical Algorithms","text":"is satisfied. In other words, we hold the entropy term constant and update (z_n y_n) in the remaining terms. The coefficients are solved efficiently through nlsolve with (z_n - 1 y_n - 1) as initial guesses.","category":"page"},{"location":"numerical_algorithms/#","page":"Numerical Algorithms","title":"Numerical Algorithms","text":"Then we compute Psi_n by solving","category":"page"},{"location":"numerical_algorithms/#","page":"Numerical Algorithms","title":"Numerical Algorithms","text":"beginaligned\n0  = Gamma_3 + Gamma_4 Psi_n + (Gamma_5 + Gamma_6 Psi_n)(Gamma_1 + Gamma_2 Psi_n) + JmathcalV(z_n - 1)\nendaligned","category":"page"},{"location":"numerical_algorithms/#","page":"Numerical Algorithms","title":"Numerical Algorithms","text":"with a Generalized Schur decomposition (also known as QZ decomposition). Notice that we also hold the Jacobian of the entropy constant. Only after we have a new round of (z_n y_n Psi_n) do we update the entropy-related terms.","category":"page"},{"location":"numerical_algorithms/#","page":"Numerical Algorithms","title":"Numerical Algorithms","text":"Convergence is achieved once (z_n y_n Psi_n) are sufficiently close under some norm. By default, we use the L^infty norm (maximum absolute error).","category":"page"},{"location":"numerical_algorithms/#homotopy-1","page":"Numerical Algorithms","title":"Homotopy Continuation","text":"","category":"section"},{"location":"numerical_algorithms/#","page":"Numerical Algorithms","title":"Numerical Algorithms","text":"When the deterministic steady state exists, it is typically an easy problem to solve numerically. We can therefore use the equations characterizing the deterministic steady state for a homotopy continuation method. Let q be the embedding parameter. Then the homotopy continuation method iteratively solves","category":"page"},{"location":"numerical_algorithms/#","page":"Numerical Algorithms","title":"Numerical Algorithms","text":"beginaligned\n0  = mu(z y) - z\n0  = xi(z y) + Gamma_5 z + Gamma_6 y + q mathcalV(z)\n0  = Gamma_3 + Gamma_4 Psi + (Gamma_5 + Gamma_6 Psi)(Gamma_1 + Gamma_2 Psi) + q JmathcalV(z)\nendaligned","category":"page"},{"location":"numerical_algorithms/#","page":"Numerical Algorithms","title":"Numerical Algorithms","text":"for the coefficients (z_q y_q Psi_q) by increasing q from 0 to 1.","category":"page"},{"location":"numerical_algorithms/#blanchard-kahn-1","page":"Numerical Algorithms","title":"Blanchard-Kahn Conditions","text":"","category":"section"},{"location":"numerical_algorithms/#","page":"Numerical Algorithms","title":"Numerical Algorithms","text":"At the end of solve!, we check the stochastic steady state found is locally unique and saddle-path stable by checking what are known as the Blanchard-Kahn conditions. Standard references for computational macroeconomics explain what these conditions are, so we defer to them (e.g. Blanchard-Kahn (1980), Klein (2000), and Sims (2002)). For the stochastic steady state, these conditions are essentially identical to the conditions for the deterministic steady state, but the Jacobian of the expectational equations to z_t also includes the Jacobian of the entropy. In the deterministic steady state, the entropy is zero, hence the Jacobian of the entropy is zero. In the stochastic steady state, the entropy is no longer zero and varies with z_t, hence the Jacobian of the expectational equations to z_t depends on entropy.","category":"page"},{"location":"numerical_algorithms/#sparsity-numerical-algo-1","page":"Numerical Algorithms","title":"Exploiting Sparsity","text":"","category":"section"},{"location":"numerical_algorithms/#","page":"Numerical Algorithms","title":"Numerical Algorithms","text":"When solving for the deterministic or stochastic steady state, this package solves a system of nonlinear equations by calling nlsolve, whose underlying algorithms typically require calculating the Jacobian of the system of nonlinear equations. For many economic models, this system is sparse because each individual equation usually depends on a small subset of the coefficients (z y Psi). To exploit this sparsity and further accelerate computation time, we can use methods implemented by SparseDiffTools.jl. For an example, please see this script.","category":"page"},{"location":"numerical_algorithms/#","page":"Numerical Algorithms","title":"Numerical Algorithms","text":"We automate the setup process by letting the user pass the keyword sparse_jacobian = true to solve!. If this keyword is true, then there are three ways to exploit sparsity.","category":"page"},{"location":"numerical_algorithms/#","page":"Numerical Algorithms","title":"Numerical Algorithms","text":"If no other keywords are passed, then solve! will attempt to determine the sparsity pattern. By default, the sparsity pattern is determined by using finite differences to calculate a Jacobian and assuming any zeros will always be zero. If the keyword sparsity_detection = true, then solve! will try using SparsityDetection.jl. Currently, the latter approach does not work with RiskAdjustedLinearizations.jl.\nThe keyword sparsity can be used to specify the sparsity pattern of the Jacobian. If colorvec is not also passed, then the matrix coloring vector is computed based on sparsity.\nThe keyword jac_cache allows the user to specify the sparsity pattern of the Jacobian and additionally pre-allocate the Jacobian's cache, which potentially achieves speed gains by avoiding extra allocations when the Jacobian function is repeatedly constructed.","category":"page"},{"location":"numerical_algorithms/#","page":"Numerical Algorithms","title":"Numerical Algorithms","text":"If solve! is called once, then the first two approaches are essentially the same. If solve! is repeatedly called (e.g. if the model's parameters are changed), then the second two approaches are strictly faster because computing the sparsity pattern or pre-allocating the Jacobian's cache only needs to be done once, as long as the system of equations does not change.","category":"page"},{"location":"numerical_algorithms/#","page":"Numerical Algorithms","title":"Numerical Algorithms","text":"To simplify using the sparsity, colorvec, and jac_cache keywords, we implement two helper functions, compute_sparsity_pattern and preallocate_jac_cache. The first function calculates sparsity and colorvec while the second ones computes jac_cache. See the docstrings below and this example for more details.","category":"page"},{"location":"numerical_algorithms/#","page":"Numerical Algorithms","title":"Numerical Algorithms","text":"Some additional caveats on these methods:","category":"page"},{"location":"numerical_algorithms/#","page":"Numerical Algorithms","title":"Numerical Algorithms","text":"Creating a cached Jacobian with automatic differentiation via ForwardColorJacCache will not work because the objective function changes in each loop of the homotopy and relaxation algorithms, so the cached Dual matrices will have information on the wrong function after a loop completes. Currently, RiskAdjustedLinearizations.jl has not implemented a way to update the information on the function required by the Dual matrices.\nIf automatic differentiation does not work with dense Jacobians due to problems with reinterpreting the chunk size, then it will also not work when using sparse Jacobians.","category":"page"},{"location":"numerical_algorithms/#Docstrings-1","page":"Numerical Algorithms","title":"Docstrings","text":"","category":"section"},{"location":"numerical_algorithms/#","page":"Numerical Algorithms","title":"Numerical Algorithms","text":"RiskAdjustedLinearizations.relaxation!\nRiskAdjustedLinearizations.homotopy!\nRiskAdjustedLinearizations.blanchard_kahn\nRiskAdjustedLinearizations.compute_sparsity_pattern\nRiskAdjustedLinearizations.preallocate_jac_cache","category":"page"},{"location":"numerical_algorithms/#RiskAdjustedLinearizations.relaxation!","page":"Numerical Algorithms","title":"RiskAdjustedLinearizations.relaxation!","text":"relaxation!(ral, x‚Çô‚Çã‚ÇÅ, Œ®‚Çô‚Çã‚ÇÅ; tol = 1e-10, max_iters = 1000, damping = .5, pnorm = Inf,\n            schur_fnct = schur!, autodiff = :central, use_anderson = false, m = 5,\n            verbose = :none, kwargs...)\n\nsolves for the coefficients (z y Psi) of a risk-adjusted linearization by the following relaxation algorithm:\n\nInitialize guesses for (z y Psi)\nDo until convergence\na) Solve for (z y) using the expectational and state transition equations and fixing Psi.\nb) Use a QZ decomposition to solve for Psi while fixing (z y).\n\nTypes:\n\nS1 <: Number\nS2 <: Real\nS3 <: Real\n\nInputs\n\nm::RiskAdjustedLinearization: object holding functions needed to calculate   the risk-adjusted linearization\nx‚Çô‚Çã‚ÇÅ::AbstractVector{S1}: initial guess for (z y)\nŒ®‚Çô‚Çã‚ÇÅ::AbstractVector{S1}: initial guess for Psi\n\nKeywords\n\ntol::S2: convergence tolerance of residual norm for relaxation algorithm\nmax_iters::Int: maximumm number of iterations\ndamping::S2: guesses are updated as the weighted average   x‚Çô = damping * proposal + (1 - damping) * x‚Çô‚Çã‚ÇÅ.\npnorm::S3: norm for residual tolerance\nschur_fnct::Function: function for calculating the Schur factorization during QZ decomposition\nautodiff::Symbol: specifies whether to use autoamtic differentiation in nlsolve   (and is the same keyword as the autodiff keyword for nlsolve)\nuse_anderson::Bool: set to true to apply Anderson acceleration to the   fixed point iteration of the relaxation algorithm\nm::Int: m coefficient if using Anderson acceleration\nsparse_jacobian::Bool = false: if true, exploit sparsity in the Jacobian in calls to nlsolve using SparseDiffTools.jl.   If jac_cache and sparsity are nothing, then relaxation! will attempt to determine the sparsity pattern.\nsparsity::Union{AbstractArray, Nothing} = nothing: sparsity pattern for the Jacobian in calls to nlsolve\ncolorvec = nothing: matrix coloring vector for sparse Jacobian in calls to nlsolve\njac_cache = nothing: pre-allocated Jacobian cache for calls to nlsolve during the numerical algorithms\nsparsity_detection::Bool = false: If true, use SparsityDetection.jl to detect sparsity pattern (only relevant if   both jac_cache and sparsity are nothing). If false,  then the sparsity pattern is   determined by using finite differences to calculate a Jacobian and assuming any zeros will always be zero.   Currently, SparsityDetection.jl fails to work.\nverbose::Symbol: verbosity of information printed out during solution.   a) :low -> statement when homotopy continuation succeeds   b) :high -> statement when homotopy continuation succeeds and for each successful iteration\n\n\n\n\n\n","category":"function"},{"location":"numerical_algorithms/#RiskAdjustedLinearizations.homotopy!","page":"Numerical Algorithms","title":"RiskAdjustedLinearizations.homotopy!","text":"homotopy!(m, x‚Çô‚Çã‚ÇÅ; step = .1, pnorm = Inf, verbose = :none, kwargs...)\n\nsolves the system of equations characterizing a risk-adjusted linearization by a homotopy method with embedding parameter q, which steps from 0 to 1, with q = 1 obtaining the true solution.\n\nCurrently, the only algorithm for choosing q is a simple uniform step search. Given a step size Delta, we solve the homotopy starting from q = Delta and increase q by Delta until q reaches 1 or passes 1 (in which case, we force q = 1).\n\nTypes:\n\nS1 <: Number\n\nInputs\n\nm::RiskAdjustedLinearization: object holding functions needed to calculate   the risk-adjusted linearization\nx‚Çô‚Çã‚ÇÅ::AbstractVector{S1}: initial guess for (z y Psi)\n\nKeywords\n\nstep::Float64: size of the uniform step from step to 1.\npnorm::Float64: norm under which to evaluate the errors after homotopy succeeds.\nsparse_jacobian::Bool = false: if true, exploit sparsity in the Jacobian in calls to nlsolve using SparseDiffTools.jl.   If jac_cache and sparsity are nothing, then homotopy! will attempt to determine the sparsity pattern.\nsparsity::Union{AbstractArray, Nothing} = nothing: sparsity pattern for the Jacobian in calls to nlsolve\ncolorvec = nothing: matrix coloring vector for sparse Jacobian in calls to nlsolve\njac_cache = nothing: pre-allocated Jacobian cache for calls to nlsolve during the numerical algorithms\nsparsity_detection::Bool = false: If true, use SparsityDetection.jl to detect sparsity pattern (only relevant if   both jac_cache and sparsity are nothing). If false,  then the sparsity pattern is   determined by using finite differences to calculate a Jacobian and assuming any zeros will always be zero.   Currently, SparsityDetection.jl fails to work.\nverbose::Symbol: verbosity of information printed out during solution.   a) :low -> statement when homotopy continuation succeeds   b) :high -> statement when homotopy continuation succeeds and for each successful iteration\n\n\n\n\n\n","category":"function"},{"location":"numerical_algorithms/#RiskAdjustedLinearizations.blanchard_kahn","page":"Numerical Algorithms","title":"RiskAdjustedLinearizations.blanchard_kahn","text":"blanchard_kahn(m::RiskAdjustedLinearization; deterministic::Bool = false, verbose::Symbol = :high)\n\nchecks the Blanchard-Kahn conditions for whether a first-order perturbation is saddle-path stable or not.\n\nIf verbose is :low or :high, a print statement will be shown if the Blanchard-Kahn conditions are satisfied.\n\n\n\n\n\n","category":"function"},{"location":"numerical_algorithms/#RiskAdjustedLinearizations.compute_sparsity_pattern","page":"Numerical Algorithms","title":"RiskAdjustedLinearizations.compute_sparsity_pattern","text":"compute_sparsity_pattern(f::Function, x::AbstractVector{<: Number}, nrow::Int;\n                         sparsity::Union{AbstractArray, Nothing} = nothing,\n                         sparsity_detection::Bool = false)\n\ncalculates the sparsity pattern of the Jacobian of the functions Œº, Œæ, and ùí±.\n\nInputs\n\nf: is the function to be differentiated, e.g. z -> ùí±(z, Œ®, (1, 2))\nx: the vector at which differentiation occurs\nnrow: specifies the number of rows of the Jacobian\n\nKeywords\n\nsparsity: sparsity pattern of the Jacobian\nsparsity_detection: if true, use SparsityDetection.jl to determine the sparsity pattern.   If false, then the sparsity pattern is determined by using automatic differentiation   to calculate a Jacobian and assuming any zeros are supposed to be zero.\n\n\n\n\n\ncompute_sparsity_pattern(m::RiskAdjustedLinearization, algorithm::Symbol; q::Float64 = .1,\n                         sparsity::Union{AbstractArray, Nothing} = nothing,\n                         sparsity_detection::Bool = false)\n\ncalculates the sparsity pattern and matrix coloring vector of the Jacobian of the nonlinear system of equations for either the deterministic or stochastic steady state, depending on which algorithm is called.\n\nKeywords\n\nq: step size for homotopy. Should satisfy 0 < q < 1 and is only required to ensure   that the sparsity pattern is correctly determined when algorithm = :homotopy   and thus the dependence of the entropy ùí± on the coefficients (z, y, Œ®) matters.\nsparsity: sparsity pattern of the Jacobian of the nonlinear system of equations\nsparsity_detection: if true, use SparsityDetection.jl to determine the sparsity pattern.   If false, then the sparsity pattern is determined by using finite differences   to calculate a Jacobian and assuming any zeros are supposed to be zero.\n\n\n\n\n\n","category":"function"},{"location":"numerical_algorithms/#RiskAdjustedLinearizations.preallocate_jac_cache","page":"Numerical Algorithms","title":"RiskAdjustedLinearizations.preallocate_jac_cache","text":"preallocate_jac_cache(m::RiskAdjustedLinearization, algorithm::Symbol; q::Float64 = .1,\n                      sparsity::Union{AbstractArray, Nothing} = nothing,\n                      sparsity_detection::Bool = false)\n\npre-allocates the cache for the Jacobian of the nonlinear system of equations for either the deterministic or stochastic steady state, depending on which algorithm is called.\n\nKeywords\n\nq: step size for homotopy. Should satisfy 0 < q < 1 and is only required to ensure   that the sparsity pattern is correctly determined when algorithm = :homotopy   and thus the dependence of the entropy ùí± on the coefficients (z, y, Œ®) matters.\nsparsity: the sparsity pattern of the Jacobian of the nonlinear system of equations\nsparsity_detection: if true, use SparsityDetection.jl to determine the sparsity pattern.   If false, then the sparsity pattern is determined by using finite differences   to calculate a Jacobian and assuming any zeros are supposed to be zero.\n\n\n\n\n\n","category":"function"},{"location":"example/#example-1","page":"Example","title":"Example","text":"","category":"section"},{"location":"example/#","page":"Example","title":"Example","text":"This example shows how to calculate the risk-adjusted linearization of the discrete-time version of the Wachter (2013) model with disaster-risk. You can run this example using the script examples/wachter_disaster_risk/example_wachter.jl. For the equivalent code in MATLAB provided by Lopez et al., see here. See List of Examples for short descriptions of and links to all examples in this package.","category":"page"},{"location":"example/#Create-a-RiskAdjustedLinearization-1","page":"Example","title":"Create a RiskAdjustedLinearization","text":"","category":"section"},{"location":"example/#Define-Nonlinear-System-1","page":"Example","title":"Define Nonlinear System","text":"","category":"section"},{"location":"example/#","page":"Example","title":"Example","text":"The user generally needs to define","category":"page"},{"location":"example/#","page":"Example","title":"Example","text":"mu: expected state transition function\nxi nonlinear terms of the expectational equations\nccgf: conditional cumulant generating function of the exogenous shocks\nLambda: function or matrix mapping endogenous risk into state transition equations\nSigma: function or matrix mapping exogenous risk into state transition equations\nGamma_5: coefficient matrix on one-period ahead expectation of state variables\nGamma_6: coefficient matrix on one-period ahead expectation of jump variables","category":"page"},{"location":"example/#","page":"Example","title":"Example","text":"The quantities mu, xi, and ccgf are always functions. The quantities Lambda and Sigma can either be functions or matrices. For example, in endowment economies like Wachter (2013), Lambda is the zero matrix since there is no endogenous risk. In other applications, Sigma may not be state-dependent and thus a constant matrix. The last two quantities Gamma_5 and Gamma_6 are always matrices. These functions do not need to have type assertions for the inputs, but if the user wants to add type assertions, then the types should be AbstractVector{T}, AbstractMatrix{T}, or AbstractArray{T} where T should allow any subtypes of Real to permit automatic differentiation, e.g. AbstractVector{T} where T <: Real. If more specific types than AbstractVector, etc., are used, then RiskAdjustedLinearization may not work properly. For these reasons, it is recommended that type assertions are not added unless necessary.","category":"page"},{"location":"example/#","page":"Example","title":"Example","text":"In addition, you need to define initial guesses for the coefficients z, y, Œ® and specify the number of exogenous shocks NŒµ. The initial guesses can be undefined if you don't want to use actual numbers yet, but you will eventually need to provide guesses in order for the nonlinear solvers to work in the numerical algorithms.","category":"page"},{"location":"example/#Instantiate-the-object-1","page":"Example","title":"Instantiate the object","text":"","category":"section"},{"location":"example/#","page":"Example","title":"Example","text":"Once you have the required quantities, simply call","category":"page"},{"location":"example/#","page":"Example","title":"Example","text":"ral = RiskAdjustedLinearization(Œº, Œõ, Œ£, Œæ, Œì‚ÇÖ, Œì‚ÇÜ, ccgf, z, y, Œ®, NŒµ)","category":"page"},{"location":"example/#Example-1","page":"Example","title":"Example","text":"","category":"section"},{"location":"example/#","page":"Example","title":"Example","text":"The following code presents a function that defines the desired functions and matrices, given the parameters for the model in Wachter (2013), and returns a RiskAdjustedLinearization object. The code is from this script examples/wachter_disaster_risk/wachter.jl, which has examples for both in-place and out-of-place functions.","category":"page"},{"location":"example/#","page":"Example","title":"Example","text":"function inplace_wachter_disaster_risk(m::WachterDisasterRisk{T}) where {T <: Real}\n    @unpack Œº‚Çê, œÉ‚Çê, ŒΩ, Œ¥, œÅ‚Çö, pp, œï‚Çö, œÅ, Œ≥, Œ≤ = m\n\n    @assert œÅ != 1. # Forcing œÅ to be non-unit for this example\n\n    S  = OrderedDict{Symbol, Int}(:p => 1,  :Œµc => 2, :ŒµŒæ => 3) # State variables\n    J  = OrderedDict{Symbol, Int}(:vc => 1, :xc => 2, :rf => 3) # Jump variables\n    SH = OrderedDict{Symbol, Int}(:Œµ‚Çö => 1, :Œµc => 2, :ŒµŒæ => 3) # Exogenous shocks\n    Nz = length(S)\n    Ny = length(J)\n    NŒµ = length(SH)\n\n    function Œº(F, z, y)\n        F_type    = eltype(F)\n        F[S[:p]]  = (1 - œÅ‚Çö) * pp + œÅ‚Çö * z[S[:p]]\n        F[S[:Œµc]] = zero(F_type)\n        F[S[:ŒµŒæ]] = zero(F_type)\n    end\n\n    function Œæ(F, z, y)\n        F[J[:vc]] = log(Œ≤) - Œ≥ * Œº‚Çê + Œ≥ * ŒΩ * z[S[:p]] - (œÅ - Œ≥) * y[J[:xc]] + y[J[:rf]]\n        F[J[:xc]] = log(1. - Œ≤ + Œ≤ * exp((1. - œÅ) * y[J[:xc]])) - (1. - œÅ) * y[J[:vc]]\n        F[J[:rf]] = (1. - Œ≥) * (Œº‚Çê - ŒΩ * z[S[:p]] - y[J[:xc]])\n    end\n\n    Œõ = zeros(T, Nz, Ny)\n\n    function Œ£(F, z)\n        F_type = eltype(F)\n        F[SH[:Œµ‚Çö], SH[:Œµ‚Çö]] = sqrt(z[S[:p]]) * œï‚Çö * œÉ‚Çê\n        F[SH[:Œµc], SH[:Œµc]] = one(F_type)\n        F[SH[:ŒµŒæ], SH[:ŒµŒæ]] = one(F_type)\n    end\n\n    function ccgf(F, Œ±, z)\n        F .= .5 .* Œ±[:, 1].^2 + .5 * Œ±[:, 2].^2 + (exp.(Œ±[:, 3] + Œ±[:, 3].^2 .* Œ¥^2 ./ 2.) .- 1. - Œ±[:, 3]) * z[S[:p]]\n    end\n\n    Œì‚ÇÖ = zeros(T, Ny, Nz)\n    Œì‚ÇÖ[J[:vc], S[:Œµc]] = (-Œ≥ * œÉ‚Çê)\n    Œì‚ÇÖ[J[:vc], S[:ŒµŒæ]] = (Œ≥ * ŒΩ)\n    Œì‚ÇÖ[J[:rf], S[:Œµc]] = (1. - Œ≥) * œÉ‚Çê\n    Œì‚ÇÖ[J[:rf], S[:ŒµŒæ]] = -(1. - Œ≥) * ŒΩ\n\n    Œì‚ÇÜ = zeros(T, Ny, Ny)\n    Œì‚ÇÜ[J[:vc], J[:vc]] = (œÅ - Œ≥)\n    Œì‚ÇÜ[J[:rf], J[:vc]] = (1. - Œ≥)\n\n    z = [pp, 0., 0.]\n    xc_sss = log((1. - Œ≤) / (exp((1. - œÅ) * (ŒΩ * pp - Œº‚Çê)) - Œ≤)) / (1. - œÅ)\n    vc_sss = xc_sss + ŒΩ * pp - Œº‚Çê\n    y = [vc_sss, xc_sss, -log(Œ≤) + Œ≥ * (Œº‚Çê - ŒΩ * pp) - (œÅ - Œ≥) * (vc_sss - xc_sss)]\n    Œ® = zeros(T, Ny, Nz)\n    return RiskAdjustedLinearization(Œº, Œõ, Œ£, Œæ, Œì‚ÇÖ, Œì‚ÇÜ, ccgf, z, y, Œ®, NŒµ)\nend","category":"page"},{"location":"example/#Solve-using-a-Newton-type-Numerical-Algorithm-1","page":"Example","title":"Solve using a Newton-type Numerical Algorithm","text":"","category":"section"},{"location":"example/#","page":"Example","title":"Example","text":"To solve the model using the relaxation algorithm, just call","category":"page"},{"location":"example/#","page":"Example","title":"Example","text":"solve!(ral; algorithm = :relaxation)","category":"page"},{"location":"example/#","page":"Example","title":"Example","text":"This form of solve! uses the coefficients in ral as initial guesses. To specify other initial guesses, call","category":"page"},{"location":"example/#","page":"Example","title":"Example","text":"solve!(ral, z0, y0, Œ®0; algorithm = :relaxation)","category":"page"},{"location":"example/#","page":"Example","title":"Example","text":"If you don't have a guess for Psi, then you can just provide guesses for z and y:","category":"page"},{"location":"example/#","page":"Example","title":"Example","text":"solve!(ral, z0, y0; algorithm = :relaxation)","category":"page"},{"location":"example/#","page":"Example","title":"Example","text":"In this case, we calculate the deterministic steady state first using z and y; back out the implied Psi; and then proceed with the relaxation algorithm using the deterministic steady state as the initial guess.","category":"page"},{"location":"example/#[Additional-Examples](@ref-listexample)-1","page":"Example","title":"Additional Examples","text":"","category":"section"},{"location":"example/#","page":"Example","title":"Example","text":"Wachter (2013): discrete-time model with Epstein-Zin preferences and disaster risk (as a Poisson mixture of normals); demonstrates how to use both in-place and out-of-place functions with RiskAdjustedLinearization\nJermann (1998)/Chen (2017): RBC model with Campbell-Cochrane habits and multi-period approximation of forward difference equations\nTextbook New Keynesian Model: cashless limit of Gali (2015) textbook model with a simple Taylor rule. A Dynare script is also provided for comparison.\nCoeurdacier, Rey, Winant (2011): small-open economy model whose deterministic steady state does not exist. Example also provides a tutorial on calculating Euler equation errors.\nNew Keynesian Model with Capital: cashless limit of Gali (2015) textbook model with capital accumulation, capital adjustment costs, additional shocks, and a Taylor rule on both inflation and output growth. A Dynare script is also provided for comparison.\nMATLAB Timing Test: compare speed of Julia to MATLAB for Jermann (1998) and Wachter (2013) examples","category":"page"}]
}
